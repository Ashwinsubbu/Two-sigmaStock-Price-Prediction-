{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Essential Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from kaggle.competitions import twosigmanews\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import keras\n",
    "from datetime import datetime, date\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Training Data from the Kaggle Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51d2ff34c8802741d38cddcba9798b153f4ae127"
   },
   "outputs": [],
   "source": [
    "env = twosigmanews.make_env()\n",
    "\n",
    "(market_train, news_train) = env.get_training_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Value Replacement  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b15dd6b682451278a866a05fef233d4ecfb2668a"
   },
   "outputs": [],
   "source": [
    "market_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc9d28008a361a03f5dc2109ccdaafe11417666a"
   },
   "outputs": [],
   "source": [
    "for columns in market_train:\n",
    "    if market_train[columns].dtype == 'int64' or market_train[columns].dtype == 'float64':\n",
    "        market_train[columns] = market_train[columns].fillna(market_train[columns].mean())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f609ac485560ee69d56cbfd9d8504e5b910b11ba"
   },
   "outputs": [],
   "source": [
    "market_train.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding of Asset Codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ac0a8d2712b30e18376a190998f4b6e76b946a6"
   },
   "outputs": [],
   "source": [
    "market_train.time = market_train.time.dt.date\n",
    "lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n",
    "market_train['assetCode'] = market_train['assetCode'].map(lbl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49b3c27c6ebdf68fa269e1d84d591ec99dd4d095"
   },
   "outputs": [],
   "source": [
    "market_train = market_train.dropna(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The entire dataset was not used, only data from (01/01/2009) was used for making the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddbb80374de262536561431b71f45ed967c275f5"
   },
   "outputs": [],
   "source": [
    "market_train = market_train.loc[market_train['time']>=date(2009, 1, 1)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Variable for our classification problem \n",
    "This binary variable returns true if market returns are positive and returns false if market returns are negative tp find which\n",
    "of these stocks are most likely to make postive returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbe1da45a89212464502b3d85f45cd8ef1c3b134"
   },
   "outputs": [],
   "source": [
    "up = (market_train.returnsOpenNextMktres10 >= 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "464fccbec568b0e9a59aa4c84324d27079e7f7a8"
   },
   "outputs": [],
   "source": [
    "y = market_train.returnsOpenNextMktres10.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "303bc1569b349f105d584a8120087d679dfd9816"
   },
   "outputs": [],
   "source": [
    "num = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n",
    "                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n",
    "                    'returnsOpenPrevMktres10'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "267c805c873341919427dc238f69565ddf1361df"
   },
   "outputs": [],
   "source": [
    "X = market_train[num] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c64c120f2883ffeb81def431dc251ec497545526"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "875323f25745ff3f32fc3dc5b0bbe6660edff84f"
   },
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0] == up.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2642335acf4ae3a4c11b8bc5850ff0924f2b946a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "scaler = MinMaxScaler() \n",
    "X = scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b865eeb87510e31c9b08dcc8a923dd7ccaa886c0"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43502d17dbd0a933abdb1a9168b1b8a503d55ebc"
   },
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e195bde204946e2d09b610a3caece62be119c63c"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, up_train, up_test, y_train, y_test= model_selection.train_test_split(X, up, y, test_size=0.33, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifical Neural Networks Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "965268a227f42001c9464603fbe5bedcec56d72f"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38262cc92eafe78a1a36ebbb51eb56700ec91f5d"
   },
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f26cf21c8133531dad7eb5eb0c46380cebd6381"
   },
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 1024, init = 'uniform', activation = 'relu', input_dim = 11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acc8b942470837a35190e982d39b2fef934bb911"
   },
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 512, init = 'uniform', activation = 'relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36101547c35d583c7a84bc5388abd7e2db814698"
   },
   "outputs": [],
   "source": [
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b38620cbed9f23d658a8b89d02642c59b7c005ef"
   },
   "outputs": [],
   "source": [
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(output_dim = 128, init = 'uniform', activation = 'relu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de51d33ae20d2dae68c75f912df54eec06925411"
   },
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "723efaa8caa8adff883a9e1b0f32c5f738a51f20"
   },
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb881ed4767e1a057b18a65cf9bdd3d2a15efc76"
   },
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, up_train, batch_size = 300, nb_epoch = 15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "449afe410c5d6da6e39545522831bc91f3ac8648"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(X_train, label= up_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "307f585029c46a541e482ec2edf9e4e65c9b3ad1"
   },
   "outputs": [],
   "source": [
    "x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_1[0],\n",
    "        'num_leaves': x_1[1],\n",
    "        'min_data_in_leaf': x_1[2],\n",
    "        'num_iteration': x_1[3],\n",
    "        'max_bin': x_1[4],\n",
    "        'verbose': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1fd919d34c0b84dce1b487bd6d0d0e665c7c315"
   },
   "outputs": [],
   "source": [
    "clf = lgb.train(params, d_train, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learing Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(clf, Learning Curve, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions using Parallel Ensemble Technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "099e133cdb3f13c422905e2b6816f62caee67513"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd694f5f322fc3bab36b9e966ecd75add9905970"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "n_days = 0\n",
    "prep_time = 0\n",
    "prediction_time = 0\n",
    "packaging_time = 0\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days +=1\n",
    "    print(n_days,end=' ')\n",
    "    t = time.time()\n",
    "    \n",
    "    for columns in market_obs_df:\n",
    "        if market_obs_df[columns].dtype == 'int64' or market_obs_df[columns].dtype == 'float64':\n",
    "            market_obs_df[columns] = market_obs_df[columns].fillna(market_obs_df[columns].mean()) \n",
    "\n",
    "    market_obs_df.time = market_obs_df.time.dt.date\n",
    "    lbl = {k: v for v, k in enumerate(market_obs_df['assetCode'].unique())}\n",
    "    market_obs_df['assetCode'] = market_obs_df['assetCode'].map(lbl) \n",
    "    \n",
    "    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n",
    "    X_live = market_obs_df[num].values\n",
    "    mins = np.min(X_live, axis=0)\n",
    "    maxs = np.max(X_live, axis=0)\n",
    "    rng = maxs - mins\n",
    "    X_live = 1 - ((maxs - X_live) / rng)\n",
    "    prep_time += time.time() - t\n",
    "    \n",
    "    t = time.time()\n",
    "    lp1 = clf.predict(X_live) \n",
    "    lp2 = classifier.predict(X_live) \n",
    "    lp = (lp1+lp2)/2\n",
    "    prediction_time += time.time() -t\n",
    "    \n",
    "    t = time.time()\n",
    "    confidence = lp\n",
    "    confidence = (confidence-confidence.min())/(confidence.max()-confidence.min())\n",
    "    confidence = confidence * 2 - 1\n",
    "    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n",
    "    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n",
    "    env.predict(predictions_template_df)\n",
    "    packaging_time += time.time() - t "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
